{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZd_rTrPYm-9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MAX_ACCOUNT_BALANCE = 2147483647\n",
        "MAX_NUM_SHARES = 2147483647\n",
        "MAX_SHARE_PRICE = 5000\n",
        "MAX_OPEN_POSITIONS = 5\n",
        "MAX_STEPS = 20000\n",
        "\n",
        "INITIAL_ACCOUNT_BALANCE = 10000"
      ],
      "metadata": {
        "id": "vIyDHkNuYx7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We first define the type and shape of our ACTION_SPACE, which will contain all of the actions possible for an agent to take in the environment.\n",
        "\n",
        "- We’ll define the OBSERVATION_SPACE, which contains all of the environment’s data to be observed by the agent.\n",
        "\n",
        "- Our RESET method will be called to periodically reset the environment to an initial state.\n",
        "\n",
        "- This is followed by many STEPS through the environment, in which an action will be provided by the model and must be executed, and the next observation returned. This is also where rewards are calculated\n",
        "\n",
        "- Finally, the RENDER method may be called periodically to print a rendition of the environment. This could be as simple as a print statement, or as complicated as rendering a 3D environment using openGL. For this example, we will stick with print statements.\n",
        "\n"
      ],
      "metadata": {
        "id": "CN9qCFhZZNML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockTradingEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "#First, we need define the action_space and observation_space in the environment’s constructor. \n",
        "#The environment expects a pandas data frame to be passed in containing the stock data to be learned from\n",
        "\n",
        "    def __init__(self, df):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
        "\n",
        "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float16)\n",
        "\n",
        "        # Prices contains the OHCL values for the last five prices\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
        "\n",
        "# The _next_observation method compiles the stock data for the last\n",
        "# five time steps, appends the agent’s account information, and scales all the values to between 0 and 1.\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
        "        frame = np.array([\n",
        "            self.df.loc[self.current_step: self.current_step +\n",
        "                        5, 'Open'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step: self.current_step +\n",
        "                        5, 'High'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step: self.current_step +\n",
        "                        5, 'Low'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step: self.current_step +\n",
        "                        5, 'Close'].values / MAX_SHARE_PRICE,\n",
        "            self.df.loc[self.current_step: self.current_step +\n",
        "                        5, 'Volume'].values / MAX_NUM_SHARES,\n",
        "        ])\n",
        "\n",
        "        # Append additional data and scale each value to between 0-1\n",
        "        obs = np.append(frame, [[\n",
        "            self.balance / MAX_ACCOUNT_BALANCE,\n",
        "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
        "            self.shares_held / MAX_NUM_SHARES,\n",
        "            self.cost_basis / MAX_SHARE_PRICE,\n",
        "            self.total_shares_sold / MAX_NUM_SHARES,\n",
        "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
        "        ]], axis=0)\n",
        "\n",
        "        return obs\n",
        "\n",
        "#Now, our _take_action method needs to take the action provided by the model and either buy, sell, or hold the stock.\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # Set the current price to a random price within the time step\n",
        "        current_price = random.uniform(\n",
        "            self.df.loc[self.current_step, \"Open\"], self.df.loc[self.current_step, \"Close\"])\n",
        "\n",
        "        action_type = action[0]\n",
        "        amount = action[1]\n",
        "\n",
        "        if action_type < 1:\n",
        "            # Buy amount % of balance in shares\n",
        "            total_possible = int(self.balance / current_price)\n",
        "            shares_bought = int(total_possible * amount)\n",
        "            prev_cost = self.cost_basis * self.shares_held\n",
        "            additional_cost = shares_bought * current_price\n",
        "\n",
        "            self.balance -= additional_cost\n",
        "            self.cost_basis = (\n",
        "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
        "            self.shares_held += shares_bought\n",
        "\n",
        "        elif action_type < 2:\n",
        "            # Sell amount % of shares held\n",
        "            shares_sold = int(self.shares_held * amount)\n",
        "            self.balance += shares_sold * current_price\n",
        "            self.shares_held -= shares_sold\n",
        "            self.total_shares_sold += shares_sold\n",
        "            self.total_sales_value += shares_sold * current_price\n",
        "\n",
        "        self.net_worth = self.balance + self.shares_held * current_price\n",
        "\n",
        "        if self.net_worth > self.max_net_worth:\n",
        "            self.max_net_worth = self.net_worth\n",
        "\n",
        "        if self.shares_held == 0:\n",
        "            self.cost_basis = 0\n",
        "\n",
        "            #Next, our environment needs to be able to take a step. \n",
        "            #At each step we will take the specified action (chosen by our model), calculate the reward, and return the next observation.\n",
        "\n",
        "    def step(self, action):\n",
        "        # Execute one time step within the environment\n",
        "        self._take_action(action)\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n",
        "            self.current_step = 0\n",
        "\n",
        "        delay_modifier = (self.current_step / MAX_STEPS)\n",
        "\n",
        "        reward = self.balance * delay_modifier\n",
        "        done = self.net_worth <= 0\n",
        "\n",
        "        obs = self._next_observation()\n",
        "\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "#Next, we’ll write the reset method, which is called any time a new environment is created or to reset an existing environment’s state. \n",
        "#It’s here where we’ll set the starting balance of each agent and initialize its open positions to an empty list.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the state of the environment to an initial state\n",
        "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
        "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
        "        self.shares_held = 0\n",
        "        self.cost_basis = 0\n",
        "        self.total_shares_sold = 0\n",
        "        self.total_sales_value = 0\n",
        "\n",
        "        # Set the current step to a random point within the data frame\n",
        "        self.current_step = random.randint(\n",
        "            0, len(self.df.loc[:, 'Open'].values) - 6)\n",
        "\n",
        "        return self._next_observation()\n",
        "\n",
        "#The only thing left to do now is render the environment to the screen. \n",
        "#For simplicity’s sake, we will just render the profit made so far and a couple other interesting metrics.\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        # Render the environment to the screen\n",
        "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
        "\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(\n",
        "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
        "        print(\n",
        "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
        "        print(\n",
        "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
        "        print(f'Profit: {profit}')"
      ],
      "metadata": {
        "id": "4VgqlRhjY2Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Our observation_space contains all of the input variables we want our agent to consider before making, or not making a trade. In this example, we want our agent to “see” the stock data points (open price, high, low, close, and daily volume) for the last five days, as well a couple other data points like its account balance, current stock positions, and current profit.\n",
        "\n",
        "- Once a trader has perceived their environment, they need to take an action. In our agent’s case, its action_space will consist of three possibilities: buy a stock, sell a stock, or do nothing.\n",
        "\n",
        "- But this isn’t enough; we need to know the amount of a given stock to buy or sell each time. Using gym’s Box space, we can create an action space that has a discrete number of action types (buy, sell, and hold), as well as a continuous spectrum of amounts to buy/sell (0-100% of the account balance/position size respectively).\n",
        "\n",
        "- The last thing to consider before implementing our environment is the reward. We want to incentivize profit that is sustained over long periods of time. At each step, we will set the reward to the account balance multiplied by some fraction of the number of time steps so far.\n",
        "\n",
        "- The _next_observation method compiles the stock data for the last five time steps, appends the agent’s account information, and scales all the values to between 0 and 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "SLVlGIpFaD2D"
      }
    }
  ]
}